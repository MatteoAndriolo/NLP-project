{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4ef0ea27f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "torch.manual_seed(99)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ArcEager Parser\n",
    "\n",
    "> The arc-eager system defines an incremental left-to-right parsing order, where left dependents are added bottom–up and right dependents top–down, which is advanta-geous for postponing certain attachment decisions. However, a fundamental problem with this system is that it **does not guarantee that the output parse is a projective dependency tree**, only a projective dependency forest, that is, a sequence of adjacent, non-overlapping projective trees (Nivre 2008). This is different from the closely related arc-standard system (Nivre 2004), which constructs all dependencies bottom–up and can easily be constrained to only output trees. The failure to implement the tree constraint may lead to fragmented parses and lower parsing accuracy, especially with respect to the global structure of the sentence. Moreover, even if the loss in accuracy is not substantial, this may be problematic when using the parser in applications where downstream components may not function correctly if the parser output is not a well-formed tree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% ArcStandardParser\n",
    "class ArcEager:\n",
    "    def __init__(self, sentence):\n",
    "        self.sentence = sentence\n",
    "        self.buffer = [i for i in range(len(self.sentence))]\n",
    "        self.stack = []\n",
    "        self.arcs = [-1 for _ in range(len(self.sentence))]\n",
    "\n",
    "        # three shift moves to initialize the stack\n",
    "        self.shift()\n",
    "        self.shift()\n",
    "        if len(self.sentence) > 2:\n",
    "            self.shift()\n",
    "\n",
    "    def shift(self):\n",
    "        b1 = self.stack.pop(0)\n",
    "        self.stack.append(b1)\n",
    "\n",
    "    def left_arc(self):\n",
    "        s1 = self.stack.pop()\n",
    "        b1 = self.buffer[0]\n",
    "        self.arcs[s1] = b1\n",
    "\n",
    "    def right_arc(self):\n",
    "        s1 = self.stack.pop()\n",
    "        b1 = self.buffer.pop(0)\n",
    "        self.arcs[b1] = s1\n",
    "        self.stack.append(b1)\n",
    "\n",
    "    def reduce(self):\n",
    "        self.stack.pop()\n",
    "\n",
    "    def is_tree_final(self):\n",
    "        return len(self.stack) == 1 and len(self.buffer) == 0\n",
    "\n",
    "    def print_configuration(self):\n",
    "        s = [self.sentence[i] for i in self.stack]\n",
    "        b = [self.sentence[i] for i in self.buffer]\n",
    "        print(s, b)\n",
    "        print(self.arcs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Shift moves next to the stack.\n",
    "2. Reduce pops the stack; allowed only if top has a head.\n",
    "3. Right-Arc adds a dependency arc from top to next and moves next to the\n",
    "   stack.\n",
    "4. Left-Arc adds a dependency arc from next to top and pops the stack;\n",
    "   allowed only if top has no head.\n",
    "\n",
    "From [here](https://aclanthology.org/J14-2002.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Oracle:\n",
    "    def __init__(self, parser, gold_tree):\n",
    "        self.parser = parser\n",
    "        self.gold = gold_tree\n",
    "\n",
    "    def is_left_arc_gold(self):\n",
    "        # first element of the of the buffer is the gold head of the topmost element of the stack\n",
    "\n",
    "        if len(self.parser.stack) == 0 or len(self.parser.buffer) == 0:\n",
    "            return False\n",
    "\n",
    "        o1 = self.parser.stack[-1]\n",
    "        o2 = self.parser.buffer[0]  # [0]\n",
    "\n",
    "        if self.gold[o2] == o1:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def is_right_arc_gold(self):\n",
    "        # if topmost stack element is gold head of the first element of the buffer\n",
    "        if len(self.parser.stack) == 0 or len(self.parser.buffer) == 0:\n",
    "            return False\n",
    "\n",
    "        o1 = self.parser.stack[-1]\n",
    "        o2 = self.parser.buffer[0]  # [0]\n",
    "\n",
    "        if self.gold[o1] == o2:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def is_reduce_gold(self):\n",
    "        # if topmost stack element has got head\n",
    "        if len(self.parser.stack) == 0:\n",
    "            return False\n",
    "\n",
    "        o1 = self.parser.stack[-1]\n",
    "\n",
    "        if self.gold[o1] != -1:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def is_shift_gold(self):\n",
    "        if len(self.parser.buffer) == 0:\n",
    "            return False\n",
    "\n",
    "        # This dictates transition precedence of the parser\n",
    "        if self.is_left_arc_gold() or self.is_right_arc_gold() or self.is_reduce_gold():\n",
    "            return False\n",
    "\n",
    "        return True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function returns whether a tree is projective or not. It is currently\n",
    "# implemented inefficiently by brute checking every pair of arcs.\n",
    "def is_projective(tree):\n",
    "    for i in range(len(tree)):\n",
    "        if tree[i] == -1:\n",
    "            continue\n",
    "        left = min(i, tree[i])\n",
    "        right = max(i, tree[i])\n",
    "\n",
    "        for j in range(0, left):\n",
    "            if tree[j] > left and tree[j] < right:\n",
    "                return False\n",
    "        for j in range(left + 1, right):\n",
    "            if tree[j] < left or tree[j] > right:\n",
    "                return False\n",
    "        for j in range(right + 1, len(tree)):\n",
    "            if tree[j] > left and tree[j] < right:\n",
    "                return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "# the function creates a dictionary of word/index pairs: our embeddings vocabulary\n",
    "# threshold is the minimum number of appearance for a token to be included in the embedding list\n",
    "def create_dict(dataset, threshold=3):\n",
    "    dic = {}  # dictionary of word counts\n",
    "    for sample in dataset:\n",
    "        for word in sample[\"tokens\"]:\n",
    "            if word in dic:\n",
    "                dic[word] += 1\n",
    "            else:\n",
    "                dic[word] = 1\n",
    "\n",
    "    map = {}  # dictionary of word/index pairs. This is our embedding list\n",
    "    map[\"<pad>\"] = 0\n",
    "    map[\"<ROOT>\"] = 1\n",
    "    map[\"<unk>\"] = 2  # used for words that do not appear in our list\n",
    "\n",
    "    next_indx = 3\n",
    "    for word in dic.keys():\n",
    "        if dic[word] >= threshold:\n",
    "            map[word] = next_indx\n",
    "            next_indx += 1\n",
    "\n",
    "    return map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset universal_dependencies (/home/matteo/.cache/huggingface/datasets/universal_dependencies/en_lines/2.7.0/1ac001f0e8a0021f19388e810c94599f3ac13cc45d6b5b8c69f7847b2188bdf7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3176\n",
      "2922 are projective\n",
      "dict_keys(['idx', 'text', 'tokens', 'lemmas', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps', 'misc'])\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"universal_dependencies\", \"en_lines\", split=\"train\")\n",
    "print(len(dataset))\n",
    "dataset = [\n",
    "    sample\n",
    "    for sample in dataset\n",
    "    if is_projective([-1] + [int(head) for head in sample[\"head\"]])\n",
    "]\n",
    "print(len(dataset), \"are projective\")\n",
    "print(dataset[1].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754\n",
      "292\n",
      "876\n"
     ]
    }
   ],
   "source": [
    "train_dataset, dev_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    dataset, [0.6, 0.1, 0.3]\n",
    ")\n",
    "# train_dataset = load_dataset(\"universal_dependencies\", \"en_lines\", split=\"train\")\n",
    "# dev_dataset = load_dataset(\"universal_dependencies\", \"en_lines\", split=\"validation\")\n",
    "# test_dataset = load_dataset(\"universal_dependencies\", \"en_lines\", split=\"test\")\n",
    "print(len(train_dataset))\n",
    "print(len(dev_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the embedding dictionary\n",
    "emb_dictionary = create_dict(train_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sample(sample, get_gold_path=False):\n",
    "    # put sentence and gold tree in our format\n",
    "    sentence = [\"<ROOT>\"] + sample[\"tokens\"]\n",
    "    gold = [-1] + [\n",
    "        int(i) for i in sample[\"head\"]\n",
    "    ]  # heads in the gold tree are strings, we convert them to int\n",
    "\n",
    "    # embedding ids of sentence words\n",
    "    enc_sentence = [\n",
    "        emb_dictionary[word] if word in emb_dictionary else emb_dictionary[\"<unk>\"]\n",
    "        for word in sentence\n",
    "    ]\n",
    "\n",
    "    # gold_path and gold_moves are parallel arrays whose elements refer to parsing steps\n",
    "    gold_path = (\n",
    "        []\n",
    "    )  # record two topmost stack tokens and first buffer token for current step\n",
    "    gold_moves = (\n",
    "        []\n",
    "    )  # contains oracle (canonical) move for current step: 0 is left, 1 right, 2 shift, 3 reduce\n",
    "\n",
    "    if get_gold_path:  # only for training\n",
    "        parser = ArcEager(sentence)\n",
    "        oracle = Oracle(parser, gold)\n",
    "\n",
    "        while not parser.is_tree_final():\n",
    "            # save configuration\n",
    "            configuration = [\n",
    "                parser.stack[len(parser.stack) - 2],\n",
    "                parser.stack[len(parser.stack) - 1],\n",
    "            ]\n",
    "            if len(parser.buffer) == 0:\n",
    "                configuration.append(-1)\n",
    "            else:\n",
    "                configuration.append(parser.buffer[0])\n",
    "            gold_path.append(configuration)\n",
    "\n",
    "            # save gold move\n",
    "            if oracle.is_left_arc_gold():\n",
    "                gold_moves.append(0)\n",
    "                parser.left_arc()\n",
    "            elif oracle.is_right_arc_gold():\n",
    "                parser.right_arc()\n",
    "                gold_moves.append(1)\n",
    "            elif oracle.is_shift_gold():\n",
    "                parser.shift()\n",
    "                gold_moves.append(2)\n",
    "            elif oracle.is_reduce_gold():\n",
    "                gold_moves.append(3)\n",
    "                parser.reduce()\n",
    "\n",
    "    return enc_sentence, gold_path, gold_moves, gold"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "def prepare_batch(batch_data, get_gold_path=False):\n",
    "    data = [process_sample(s, get_gold_path=get_gold_path) for s in batch_data]\n",
    "    # sentences, paths, moves, trees are parallel arrays, each element refers to a sentence\n",
    "    sentences = [s[0] for s in data]\n",
    "    paths = [s[1] for s in data]\n",
    "    moves = [s[2] for s in data]\n",
    "    trees = [s[3] for s in data]\n",
    "    return sentences, paths, moves, trees\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=partial(prepare_batch, get_gold_path=True),\n",
    ")\n",
    "dev_dataloader = torch.utils.data.DataLoader(\n",
    "    dev_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=partial(prepare_batch)\n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=partial(prepare_batch),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HighLevelStructure BiLSTM\n",
    "\n",
    "1. WordEmbeddings - done before, DataLoaders done\n",
    "2. BiLSTM Layer\n",
    "3. Parser Configuration Representation\n",
    "4. Action Classifier\n",
    "5. Training\n",
    "6. Prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed sample\n",
    "\n",
    "Generate all possible buffer-stack configurations with the gold output attached\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # put sentence and gold tree in our format\n",
    "    sentence = [\"<ROOT>\"] + sample[\"tokens\"]\n",
    "    gold = [-1] + [\n",
    "        int(i) for i in sample[\"head\"]\n",
    "    ]  # heads in the gold tree are strings, we convert them to int\n",
    "\n",
    "    # embedding ids of sentence words\n",
    "    enc_sentence = [\n",
    "        emb_dictionary[word] if word in emb_dictionary else emb_dictionary[\"<unk>\"]\n",
    "        for word in sentence\n",
    "    ]\n",
    "\n",
    "    # gold_path and gold_moves are parallel arrays whose elements refer to parsing steps\n",
    "    gold_path = (\n",
    "        []\n",
    "    )  # record two topmost stack tokens and first buffer token for current step\n",
    "    gold_moves = (\n",
    "        []\n",
    "    )  # contains oracle (canonical) move for current step: 0 is left, 1 right, 2 shift\n",
    "\n",
    "    if get_gold_path:  # only for training\n",
    "        parser = ArcEager(sentence)\n",
    "        oracle = Oracle(parser, gold)\n",
    "\n",
    "        while not parser.is_tree_final():\n",
    "            # save configuration\n",
    "            configuration = [\n",
    "                parser.stack[len(parser.stack) - 2],\n",
    "                parser.stack[len(parser.stack) - 1],\n",
    "            ]\n",
    "            if len(parser.buffer) == 0:\n",
    "                configuration.append(-1)\n",
    "            else:\n",
    "                configuration.append(parser.buffer[0])\n",
    "            gold_path.append(configuration)\n",
    "\n",
    "            # save gold move\n",
    "            if oracle.is_reduce_gold():\n",
    "                gold_moves.append(0)\n",
    "                parser.reduce()\n",
    "            if oracle.is_left_arc_gold():\n",
    "                gold_moves.append(1)\n",
    "                parser.left_arc()\n",
    "            elif oracle.is_right_arc_gold():\n",
    "                parser.right_arc()\n",
    "                gold_moves.append(2)\n",
    "            elif oracle.is_shift_gold():\n",
    "                parser.shift()\n",
    "                gold_moves.append(3)\n",
    "\n",
    "    return enc_sentence, gold_path, gold_moves, gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batch(batch_data, get_gold_path=False):\n",
    "    data = [process_sample(s, get_gold_path=get_gold_path) for s in batch_data]\n",
    "    # sentences, paths, moves, trees are parallel arrays, each element refers to a sentence\n",
    "    sentences = [s[0] for s in data]\n",
    "    paths = [s[1] for s in data]\n",
    "    moves = [s[2] for s in data]\n",
    "    trees = [s[3] for s in data]\n",
    "    return sentences, paths, moves, trees\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=partial(prepare_batch, get_gold_path=True),\n",
    ")\n",
    "dev_dataloader = torch.utils.data.DataLoader(\n",
    "    dev_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=partial(prepare_batch)\n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=partial(prepare_batch),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% CREATE NN MODEL\n",
    "\n",
    "EMBEDDING_SIZE = 200\n",
    "LSTM_SIZE = 200\n",
    "LSTM_LAYERS = 1\n",
    "MLP_SIZE = 200\n",
    "DROPOUT = 0.2\n",
    "EPOCHS = 15\n",
    "LR = 0.001  # learning rate\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(Net, self).__init__()\n",
    "        self.device = device\n",
    "        self.embeddings = nn.Embedding(\n",
    "            len(emb_dictionary), EMBEDDING_SIZE, padding_idx=emb_dictionary[\"<pad>\"]\n",
    "        )\n",
    "\n",
    "        # initialize bi-LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            EMBEDDING_SIZE,\n",
    "            LSTM_SIZE,\n",
    "            num_layers=LSTM_LAYERS,\n",
    "            bidirectional=True,\n",
    "            dropout=DROPOUT,\n",
    "        )\n",
    "\n",
    "        # initialize feedforward\n",
    "        self.w1 = torch.nn.Linear(6 * LSTM_SIZE, MLP_SIZE, bias=True)\n",
    "        self.activation = torch.nn.Tanh()\n",
    "        self.w2 = torch.nn.Linear(MLP_SIZE, 3, bias=True)\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(DROPOUT)\n",
    "\n",
    "    def forward(self, x, paths):\n",
    "        # get the embeddings\n",
    "        x = [self.dropout(self.embeddings(torch.tensor(i).to(self.device))) for i in x]\n",
    "\n",
    "        # run the bi-lstm\n",
    "        h = self.lstm_pass(x)\n",
    "\n",
    "        # for each parser configuration that we need to score we arrange from the\n",
    "        # output of the bi-lstm the correct input for the feedforward\n",
    "        mlp_input = self.get_mlp_input(paths, h)\n",
    "\n",
    "        # run the feedforward and get the scores for each possible action\n",
    "        out = self.mlp(mlp_input)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def lstm_pass(self, x):\n",
    "        x = torch.nn.utils.rnn.pack_sequence(x, enforce_sorted=False)\n",
    "        h, (h_0, c_0) = self.lstm(x)\n",
    "        h, h_sizes = torch.nn.utils.rnn.pad_packed_sequence(\n",
    "            h\n",
    "        )  # size h: (length_sentences, batch, output_hidden_units)\n",
    "        return h\n",
    "\n",
    "    def get_mlp_input(self, configurations, h):\n",
    "        mlp_input = []\n",
    "        zero_tensor = torch.zeros(2 * LSTM_SIZE, requires_grad=False).to(self.device)\n",
    "        for i in range(len(configurations)):  # for every sentence in the batch\n",
    "            for j in configurations[i]:  # for each configuration of a sentence\n",
    "                mlp_input.append(\n",
    "                    torch.cat(\n",
    "                        [\n",
    "                            zero_tensor if j[0] == -1 else h[j[0]][i],\n",
    "                            zero_tensor if j[1] == -1 else h[j[1]][i],\n",
    "                            zero_tensor if j[2] == -1 else h[j[2]][i],\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "        mlp_input = torch.stack(mlp_input).to(self.device)\n",
    "        return mlp_input\n",
    "\n",
    "    def mlp(self, x):\n",
    "        return self.softmax(\n",
    "            self.w2(self.dropout(self.activation(self.w1(self.dropout(x)))))\n",
    "        )\n",
    "\n",
    "    # we use this function at inference time. We run the parser and at each step\n",
    "    # we pick as next move the one with the highest score assigned by the model\n",
    "    def infere(self, x):\n",
    "        parsers = [ArcStandard(i) for i in x]\n",
    "\n",
    "        x = [self.embeddings(torch.tensor(i).to(self.device)) for i in x]\n",
    "\n",
    "        h = self.lstm_pass(x)\n",
    "\n",
    "        while not self.parsed_all(parsers):\n",
    "            # get the current configuration and score next moves\n",
    "            configurations = self.get_configurations(parsers)\n",
    "            mlp_input = self.get_mlp_input(configurations, h)\n",
    "            mlp_out = self.mlp(mlp_input)\n",
    "            # take the next parsing step\n",
    "            self.parse_step(parsers, mlp_out)\n",
    "\n",
    "        # return the predicted dependency tree\n",
    "        return [parser.arcs for parser in parsers]\n",
    "\n",
    "    def get_configurations(self, parsers):\n",
    "        configurations = []\n",
    "\n",
    "        for parser in parsers:\n",
    "            if parser.is_tree_final():\n",
    "                conf = [-1, -1, -1]\n",
    "            else:\n",
    "                conf = [\n",
    "                    parser.stack[len(parser.stack) - 2],\n",
    "                    parser.stack[len(parser.stack) - 1],\n",
    "                ]\n",
    "                if len(parser.buffer) == 0:\n",
    "                    conf.append(-1)\n",
    "                else:\n",
    "                    conf.append(parser.buffer[0])\n",
    "            configurations.append([conf])\n",
    "\n",
    "        return configurations\n",
    "\n",
    "    def parsed_all(self, parsers):\n",
    "        for parser in parsers:\n",
    "            if not parser.is_tree_final():\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    # In this function we select and perform the next move according to the scores obtained.\n",
    "    # We need to be careful and select correct moves, e.g. don't do a shift if the buffer\n",
    "    # is empty or a left arc if σ2 is the ROOT. For clarity sake we didn't implement\n",
    "    # these checks in the parser so we must do them here. This renders the function quite ugly\n",
    "    def parse_step(self, parsers, moves):\n",
    "        moves_argm = moves.argmax(-1)\n",
    "        for i in range(len(parsers)):\n",
    "            if parsers[i].is_tree_final():\n",
    "                continue\n",
    "            else:\n",
    "                if moves_argm[i] == 0:\n",
    "                    if parsers[i].stack[len(parsers[i].stack) - 2] != 0:\n",
    "                        parsers[i].left_arc()\n",
    "                    else:\n",
    "                        if len(parsers[i].buffer) > 0:\n",
    "                            parsers[i].shift()\n",
    "                        else:\n",
    "                            parsers[i].right_arc()\n",
    "                elif moves_argm[i] == 1:\n",
    "                    if (\n",
    "                        parsers[i].stack[len(parsers[i].stack) - 2] == 0\n",
    "                        and len(parsers[i].buffer) > 0\n",
    "                    ):\n",
    "                        parsers[i].shift()\n",
    "                    else:\n",
    "                        parsers[i].right_arc()\n",
    "                elif moves_argm[i] == 2:\n",
    "                    if len(parsers[i].buffer) > 0:\n",
    "                        parsers[i].shift()\n",
    "                    else:\n",
    "                        if moves[i][0] > moves[i][1]:\n",
    "                            if parsers[i].stack[len(parsers[i].stack) - 2] != 0:\n",
    "                                parsers[i].left_arc()\n",
    "                            else:\n",
    "                                parsers[i].right_arc()\n",
    "                        else:\n",
    "                            parsers[i].right_arc()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiLSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 20])\n"
     ]
    }
   ],
   "source": [
    "# from https://pytorch.org/docs/stable/generated/torch.nn.LSTM\n",
    "rnn = nn.LSTM(10, 20, 2)\n",
    "input = torch.randn(5, 3, 10)\n",
    "h0 = torch.randn(2, 3, 20)\n",
    "c0 = torch.randn(2, 3, 20)\n",
    "output, (hn, cn) = rnn(input, (h0, c0))\n",
    "print(output.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, hidden_size, num_layers, batch_first=True, bidirectional=True\n",
    "        )\n",
    "        self.fc = nn.Linear(\n",
    "            hidden_size * 2, output_size\n",
    "        )  # Multiply hidden_size by 2 for bidirectional LSTM\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(\n",
    "            x.device\n",
    "        )  # Initialize hidden state for forward LSTM\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(\n",
    "            x.device\n",
    "        )  # Initialize cell state for forward LSTM\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))  # Apply LSTM\n",
    "\n",
    "        # Extract the last hidden state from the bidirectional LSTM output\n",
    "        out = self.fc(out[:, -1, :])  # Shape: (batch_size, output_size)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0845,  0.1244,  0.0795, -0.0587, -0.0564],\n",
      "        [ 0.0853,  0.0852,  0.0466, -0.0511, -0.0886],\n",
      "        [ 0.0826,  0.0876,  0.0281, -0.0715, -0.0818]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the BiLSTM model\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "num_layers = 2\n",
    "output_size = 5\n",
    "\n",
    "model = BiLSTM(input_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "# Generate some sample input\n",
    "batch_size = 3\n",
    "sequence_length = 4\n",
    "input_data = torch.randn(batch_size, sequence_length, input_size)\n",
    "\n",
    "# Pass the input through the model\n",
    "output = model(input_data)\n",
    "\n",
    "print(output)\n",
    "print(output.shape)  # Print the shape of the output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from datasets import load_dataset\n",
    "\n",
    "mrpc_dataset = load_dataset(\"glue\", \"mrpc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['sentence1', 'sentence2', 'label', 'idx'])\n",
      "\n",
      "sentence1 Value(dtype='string', id=None)\n",
      "sentence2 Value(dtype='string', id=None)\n",
      "label ClassLabel(names=['not_equivalent', 'equivalent'], id=None)\n",
      "idx Value(dtype='int32', id=None)\n"
     ]
    }
   ],
   "source": [
    "print(mrpc_dataset[\"train\"].features.keys(), end=\"\\n\\n\")\n",
    "\n",
    "for k, v in mrpc_dataset[\"train\"].features.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .', 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .', 'label': 1, 'idx': 0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(mrpc_dataset[\"train\"][0], end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 3668\n",
      "validation 408\n",
      "test 1725\n"
     ]
    }
   ],
   "source": [
    "for k, v in mrpc_dataset.items():\n",
    "    print(k, len(v))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the entire dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "def tokenize(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/matteo/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab99d5d589414d098209a0526254ee59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/matteo/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-7bc4744dbc6861ed.arrow\n",
      "Loading cached processed dataset at /home/matteo/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-54ec9e7ee41eb863.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4f73023b53c4a1eae6da1364a629435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train_dataset = mprpc_dataset[\"train\"].map(tokenize, batched=True)\n",
    "\n",
    "mrpc_dataset = load_dataset(\"glue\", \"mrpc\")\n",
    "mrpc_dataset = mrpc_dataset.map(\n",
    "    tokenize, batched=True\n",
    ")  # adds input_ids, attention_mask, token_type_ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic Padding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'token_type_ids', 'labels'])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "samples = mrpc_dataset[\"train\"][:8]\n",
    "samples = {\n",
    "    \"input_ids\": samples[\"input_ids\"],  # requires tokenized words\n",
    "    \"attention_mask\": samples[\"attention_mask\"],\n",
    "    \"token_type_ids\": samples[\"token_type_ids\"],\n",
    "    \"label\": samples[\"label\"],\n",
    "}\n",
    "mrpc_dataset = data_collator(samples)  # Padding\n",
    "mrpc_dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", num_labels=2\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"bert_mrpc\",  # where to save checkpoints and model predictions\n",
    "    per_device_train_batch_size=8,  # training batch size\n",
    "    per_device_eval_batch_size=16,  # validation/test batch sizie\n",
    "    num_train_epochs=3,  # number of epochs\n",
    "    save_strategy=\"epoch\",  # checkpoint saving frequency\n",
    "    evaluation_strategy=\"epoch\",  # how frequently to run validation, can be epoch or steps (in this case you need to specify eval_steps)\n",
    "    metric_for_best_model=\"f1\",  # metric used to pick checkpoints\n",
    "    greater_is_better=True,  # whether the metric for checkpoint needs to be maximized or minimized\n",
    "    learning_rate=3e-5,  # learning rate or peak learning rate if scheduler is used\n",
    "    optim=\"adamw_torch\",  # which optimizer to use\n",
    "    lr_scheduler_type=\"linear\",  # which scheduler to use\n",
    "    warmup_ratio=0.1,  # % of steps for which to do warmup\n",
    "    seed=33,  # setting seed for reproducibility\n",
    "    load_best_model_at_end=True,\n",
    ")  # after training, load the best checkpoint according to metric_for_best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 18\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m# %%\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m Trainer\n\u001b[1;32m     15\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     16\u001b[0m     model,\n\u001b[1;32m     17\u001b[0m     training_args,\n\u001b[0;32m---> 18\u001b[0m     train_dataset\u001b[39m=\u001b[39mmrpc_dataset[\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     19\u001b[0m     eval_dataset\u001b[39m=\u001b[39mmrpc_dataset[\u001b[39m\"\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     20\u001b[0m     data_collator\u001b[39m=\u001b[39mdata_collator,\n\u001b[1;32m     21\u001b[0m     tokenizer\u001b[39m=\u001b[39mtokenizer,\n\u001b[1;32m     22\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_metrics_mrpc,\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[39m# %%\u001b[39;00m\n\u001b[1;32m     26\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:238\u001b[0m, in \u001b[0;36mBatchEncoding.__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[39mIf the key is a string, returns the value of the dict associated to `key` ('input_ids', 'attention_mask',\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[39metc.).\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \n\u001b[1;32m    235\u001b[0m \u001b[39mIf the key is an integer, get the `tokenizers.Encoding` for batch item with index `key`.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(item, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> 238\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata[item]\n\u001b[1;32m    239\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encodings \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    240\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encodings[item]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'train'"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics_mrpc(eval_preds):\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "# %%\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=mrpc_dataset[\"train\"],\n",
    "    eval_dataset=mrpc_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics_mrpc,\n",
    ")\n",
    "\n",
    "# %%\n",
    "trainer.train()\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Computing performance on the test set\n",
    "\n",
    "# %%\n",
    "test_predictions = trainer.predict(mrpc_dataset[\"test\"])\n",
    "print(test_predictions.metrics)\n",
    "\n",
    "# %%\n",
    "trainer.state.best_model_checkpoint  # folder where best model is save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
