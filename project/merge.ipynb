{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIBRARY\n",
    "This code snippet sets up the necessary environment for deep learning with PyTorch, including GPU device selection, random seed initialization, and importing relevant libraries for working with pre-trained models in NLP tasks. Also important labraries for the usage of Arc Eager Parser and Oracle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f3230682850>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import sort as tsort, Tensor\n",
    "import time\n",
    "from typing import List, Tuple\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "from datasets import load_dataset\n",
    "from typing import List\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "torch.manual_seed(99)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arc Eager Parser\n",
    "\n",
    "The code provides an implementation of the Arc Eager parsing algorithm. The Arc Eager algorithm is used to parse sentences and build dependency tree structures.\n",
    "\n",
    "## Constants\n",
    "The code defines several constants used in the Arc Eager algorithm:\n",
    "- `NOMOVE`: Represents no move.\n",
    "- `LEFT_ARC`: Represents the left-arc move.\n",
    "- `RIGHT_ARC`: Represents the right-arc move.\n",
    "- `REDUCE`: Represents the reduce move.\n",
    "- `SHIFT`: Represents the shift move.\n",
    "- `IS_FINAL`: Represents the value indicating that the parsing process is finished.\n",
    "- `EMPTY`: Represents an empty value.\n",
    "\n",
    "## Class: ArcEager\n",
    "The `ArcEager` class represents an instance of the Arc Eager parser.\n",
    "\n",
    "### Initialization\n",
    "The `ArcEager` class takes a sentence as input and initializes the parsing process. The first element of the sentence must be `ROOT` for a list of words or 1 for a list of integers. The class maintains important data structures: `sentence`, `buffer`, `stack`, `list_arcs`, `list_moves`, and `list_configurations` to keep track of the parsing state and actions.\n",
    "\n",
    "### Methods\n",
    "1. `update_configurations`: Updates the `list_configurations` and `list_moves` based on the current move.\n",
    "2. `left_arc`: Performs the left-arc move by popping the top element from the stack and assigning it as the head of the first element in the buffer.\n",
    "3. `right_arc`: Performs the right-arc move by assigning the first element in the buffer as the head of the top element in the stack and shifting the first element from the buffer to the stack.\n",
    "4. `shift`: Performs the shift move by shifting the first element from the buffer to the stack.\n",
    "5. `reduce`: Performs the reduce move by popping the top element from the stack.\n",
    "6. `nomove`: Performs the no-move operation, indicating that the parsing process is finished.\n",
    "7. `do_move`: Performs the specified move by calling the corresponding method based on the move value.\n",
    "8. `is_tree_final`: Checks if the tree is in its final state, indicating that the parsing process is finished.\n",
    "9. `print_configuration`: Prints the current configuration of the parser, including the stack, buffer, and list of arcs.\n",
    "10. `get_list_moves`: Returns the list of moves performed during the parsing process.\n",
    "11. `get_list_configurations`: Returns the list of configurations encountered during the parsing process.\n",
    "12. `get_list_arcs`: Returns the list of arcs in the dependency tree.\n",
    "13. `get_configuration_now`: Returns the current configuration of the parser.\n",
    "\n",
    "The `ArcEager` class provides functionality to perform the Arc Eager parsing algorithm, including shifting, reducing, and creating arcs based on the input sentence.\n",
    "\n",
    "## Usage\n",
    "To use the Arc Eager parser, create an instance of the `ArcEager` class, passing the sentence as input. Then, you can perform parsing moves by calling the appropriate methods (`left_arc`, `right_arc`, `shift`, `reduce`, or `nomove`). The current state of the parser can be obtained using the provided getter methods (`get_list_moves`, `get_list_configurations`, `get_list_arcs`, `get_configuration_now`), and the parser's configuration can be printed using the `print_configuration` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOMOVE = -1\n",
    "LEFT_ARC = 0\n",
    "RIGHT_ARC = 1\n",
    "REDUCE = 2\n",
    "SHIFT = 3\n",
    "\n",
    "IS_FINAL = -10\n",
    "EMPTY = -1\n",
    "\n",
    "class ArcEager:\n",
    "    def __init__(self, sentence):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            sentence: list of words | first word must be <ROOT>\n",
    "            debug: if True print each move\n",
    "        \"\"\"\n",
    "        if all([isinstance(x,str) for x in sentence]):\n",
    "            if sentence[0] != \"<ROOT>\":\n",
    "                raise Exception(\"ERROR: first word must be <ROOT>\")\n",
    "        elif all([isinstance(x,int) for x in sentence]):\n",
    "            if sentence[0] != 1: # token of ROOT is 1\n",
    "                raise Exception(\"ERROR: first word must be -1\")\n",
    "        else:\n",
    "            raise Exception(\"ERROR: sentence must be list of words or list of ints\")\n",
    "            \n",
    "        self.sentence = sentence\n",
    "        self.buffer = [i for i in range(len(self.sentence))]\n",
    "        self.stack = []\n",
    "\n",
    "        self.list_arcs = [-1 for _ in range(len(self.sentence))]\n",
    "        self.list_moves=[]\n",
    "        self.list_configurations = []\n",
    "\n",
    "        # Do first shift -> add ROOT to stack\n",
    "        self.stack.append(self.buffer.pop(0))\n",
    "        self.is_finished=False\n",
    "\n",
    "    def update_configurations(self, move):\n",
    "        ''' to do before each move '''\n",
    "        if move == NOMOVE:\n",
    "            self.list_configurations.append([EMPTY, EMPTY])\n",
    "            self.list_moves.append(NOMOVE)\n",
    "        if len(self.stack)>0:\n",
    "            self.list_configurations.append([\n",
    "                self.stack[-1],\n",
    "                self.buffer[0] if len(self.buffer)>0 else EMPTY\n",
    "            ])\n",
    "            self.list_moves.append(move)\n",
    "            \n",
    "        \n",
    "    def left_arc(self):\n",
    "        self.update_configurations(LEFT_ARC)\n",
    "        s1 = self.stack.pop(-1)\n",
    "        b1 = self.buffer[0]\n",
    "        self.list_arcs[s1] = b1\n",
    "\n",
    "    def right_arc(self):\n",
    "        if not is_right_possible(self): \n",
    "            self.nomove()\n",
    "            return\n",
    "        self.update_configurations(RIGHT_ARC)\n",
    "        s1 = self.stack[-1]\n",
    "        b1 = self.buffer.pop(0)\n",
    "        self.stack.append(b1)\n",
    "        self.list_arcs[b1] = s1\n",
    "\n",
    "    def shift(self):\n",
    "        self.update_configurations(SHIFT)\n",
    "        self.stack.append(self.buffer.pop(0))\n",
    "\n",
    "    def reduce(self):\n",
    "        self.update_configurations(REDUCE)\n",
    "        self.stack.pop()\n",
    "\n",
    "    def nomove(self):\n",
    "        self.is_finished=True\n",
    "        self.update_configurations(NOMOVE)\n",
    "\n",
    "    def do_move(self, move:int):\n",
    "        if move==LEFT_ARC: \n",
    "            self.left_arc()\n",
    "        elif move==RIGHT_ARC:\n",
    "            self.right_arc()\n",
    "        elif move==SHIFT:\n",
    "            self.shift()\n",
    "        elif move==REDUCE:\n",
    "            self.reduce()\n",
    "        elif move==NOMOVE:\n",
    "            self.nomove()\n",
    "        return move\n",
    "\n",
    "    def is_tree_final(self):\n",
    "        return self.is_finished or (len(self.stack) == 1 and len(self.buffer) == 0) \n",
    "\n",
    "    def print_configuration(self):\n",
    "        s = [self.sentence[i] for i in self.stack]\n",
    "        b = [self.sentence[i] for i in self.buffer]\n",
    "        print(s, b)\n",
    "        print(self.stack, self.buffer)\n",
    "        print(self.list_arcs)\n",
    "\n",
    "    def get_list_moves(self):\n",
    "        return self.list_moves\n",
    "\n",
    "    def get_list_configurations(self):\n",
    "        return self.list_configurations\n",
    "\n",
    "    def get_list_arcs(self):\n",
    "        return self.list_arcs\n",
    "    \n",
    "    def get_configuration_now(self):\n",
    "        if self.is_tree_final():\n",
    "            conf=[-1,-1]\n",
    "        else:\n",
    "            conf=[self.stack[-1]]\n",
    "            if len(self.buffer) == 0:\n",
    "                conf.append(-1)\n",
    "            else:\n",
    "                conf.append(self.buffer[0])\n",
    "        return conf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oracle for Arc Eager Parser\n",
    "\n",
    "The code provides an implementation of an oracle for the Arc Eager parsing algorithm. The oracle is used to guide the parsing process by determining the next move based on the gold standard tree and the current state of the parser.\n",
    "\n",
    "## Class: Oracle\n",
    "The `Oracle` class represents the oracle for the Arc Eager parser.\n",
    "\n",
    "### Initialization\n",
    "The `Oracle` class takes a parser instance and a gold tree as input and initializes the oracle. It checks the correctness of the gold tree input and stores the parser and gold tree information.\n",
    "\n",
    "### Methods\n",
    "1. `is_left_arc_gold`: Checks if there is a link from the top of the buffer to the top of the stack in the gold tree. If a link is present, it returns `True`; otherwise, it returns `False`.\n",
    "2. `is_right_arc_gold`: Checks if there is a link from the top of the stack to the top of the buffer in the gold tree. If a link is present, it returns `True`; otherwise, it returns `False`.\n",
    "3. `is_reduce_gold`: Checks if there is a link from a lower position (k) to the top of the buffer or the top of the stack (j), where k is less than the top of the stack (i), in the gold tree. If such a link exists, it returns `False`; otherwise, it returns `True`.\n",
    "4. `is_shift_gold`: Checks if there is no link from the top of the buffer to the top of the stack, no link from the top of the stack to the top of the buffer, and no link from a lower position to the top of the buffer or the top of the stack in the gold tree. If all conditions are met, it returns `True`; otherwise, it returns `False`.\n",
    "5. `get_next_move`: Determines the next move based on the current state of the parser and the gold tree. If the parsing process is finished, it returns the constant value `IS_FINAL`. If a left-arc move is possible, it returns the constant value `LEFT_ARC`. If a right-arc move is possible, it returns the constant value `RIGHT_ARC`. If a reduce move is possible, it returns the constant value `REDUCE`. If a shift move is possible, it returns the constant value `SHIFT`. If none of the conditions are met, it prints debug information, including the gold tree and the parser's list of arcs, and exits the program.\n",
    "\n",
    "The `Oracle` class helps determine the next move in the Arc Eager parsing algorithm based on the gold tree and the current state of the parser.\n",
    "\n",
    "## Usage\n",
    "To use the oracle, create an instance of the `Oracle` class, passing the parser and the gold tree as arguments. Then, call the `get_next_move` method to obtain the next move for the parser.\n",
    "\n",
    "The oracle guides the parsing process by selecting the appropriate move based on the gold standard tree, ensuring that the parser follows the correct syntactic structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Oracle:\n",
    "    def __init__(self, parser, gold_tree:List[int]):\n",
    "        self.parser = parser\n",
    "        self.gold = list(map(int,gold_tree))\n",
    "\n",
    "        # Check correctness of input\n",
    "        if self.gold[0] != -1:\n",
    "            print(\"ERROR: gold tree must start with -1\")\n",
    "            exit(-1)\n",
    "\n",
    "    \"\"\"\n",
    "    i: top of stack, j: top of buffer\n",
    "    if there's a link j -> i then return LEFT-ARC\n",
    "    else if there's a link i -> j then return RIGHT-ARC\n",
    "    else if there's a link k <-/-> j, k < i then return REDUCE\n",
    "    else return SHIFT \n",
    "    \"\"\"\n",
    "    def is_left_arc_gold(self):\n",
    "        # first element of the of the buffer is the gold head of the topmost element of the stack\n",
    "        # if empty lists or if top has no head -> return False\n",
    "        if (\n",
    "            len(self.parser.buffer) == 0\n",
    "            or self.parser.stack[-1] == 0  # if top is ROOT\n",
    "        ):\n",
    "            return False\n",
    "\n",
    "        s = self.parser.stack[-1]\n",
    "        b = self.parser.buffer[0]  # [0]\n",
    "        if self.gold[s] != b:\n",
    "            return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def is_right_arc_gold(self):\n",
    "        # if topmost stack element is gold head of the first element of the buffer\n",
    "        if len(self.parser.buffer) == 0:\n",
    "            return False\n",
    "\n",
    "        s = self.parser.stack[-1]\n",
    "        b = self.parser.buffer[0]  # [0]\n",
    "        if self.gold[b] != s:\n",
    "            return False \n",
    "\n",
    "        return True \n",
    "\n",
    "    def is_reduce_gold(self):\n",
    "        s = self.parser.stack[-1]\n",
    "        if self.parser.list_arcs[s] == -1 or s==0: # if top has no head or if top is ROOT\n",
    "            return False\n",
    "        if len(self.parser.buffer) == 0:                    # if buffer is empty\n",
    "            if self.parser.list_arcs[s] != -1 and s != 0:   # if top has a head and top is not ROOT\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "        for i in range(0, len(self.parser.buffer)):\n",
    "            b = self.parser.buffer[i]\n",
    "            if self.gold[b] == s or self.gold[s] == b: # if there's a link k <-/-> j, k < i then do not reduce\n",
    "                return False \n",
    "            \n",
    "        return True \n",
    "\n",
    "    def is_shift_gold(self):\n",
    "        if len(self.parser.buffer) == 0:\n",
    "            return False\n",
    "        if self.is_left_arc_gold() or self.is_right_arc_gold() or self.is_reduce_gold():\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def get_next_move(self, do_it=False):\n",
    "        if self.parser.is_tree_final():\n",
    "            return IS_FINAL\n",
    "        if self.is_left_arc_gold():\n",
    "            return LEFT_ARC\n",
    "        elif self.is_right_arc_gold():\n",
    "            return RIGHT_ARC\n",
    "        elif self.is_reduce_gold():\n",
    "            return REDUCE\n",
    "        elif self.is_shift_gold():\n",
    "            return SHIFT\n",
    "        else:\n",
    "            print(\"NO MOVE\")\n",
    "            print(self.gold)\n",
    "            print(self.parser.list_arcs)\n",
    "            self.parser.print_configuration()\n",
    "            exit(-5)\n",
    "            return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions\n",
    "- `is_left_possible(parser)`: This function checks if a left-arc move is possible in the current parser state. It returns `True` if the stack and buffer have at least one element each, and the top element of the stack is not the ROOT element (0). Otherwise, it returns `False`.\n",
    "\n",
    "- `is_right_possible(parser)`: This function checks if a right-arc move is possible in the current parser state. It returns `True` if the stack and buffer have at least one element each. Otherwise, it returns `False`.\n",
    "\n",
    "- `is_shift_possible(parser)`: This function checks if a shift move is possible in the current parser state. It returns `True` if the buffer has at least one element. Otherwise, it returns `False`.\n",
    "\n",
    "- `is_reduce_possible(parser)`: This function checks if a reduce move is possible in the current parser state. It returns `True` if the stack has at least one element and the top element of the stack has a head in the list of arcs. Otherwise, it returns `False`.\n",
    "\n",
    "These helper functions provide the necessary conditions for determining whether a specific move is valid given the current state of the parser.\n",
    "\n",
    "You can use these helper functions within the `ArcEager` class methods, such as `left_arc`, `right_arc`, `shift`, and `reduce`, to determine if the corresponding move is possible before performing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_left_possible(parser):\n",
    "    return len(parser.stack) >= 1 and len(parser.buffer) >= 1 and parser.stack[-1] != 0\n",
    "\n",
    "def is_right_possible(parser):\n",
    "    return len(parser.stack) >= 1 and len(parser.buffer) >= 1\n",
    "\n",
    "def is_shift_possible(parser):\n",
    "    return len(parser.buffer) >= 1\n",
    "\n",
    "def is_reduce_possible(parser):\n",
    "    return len(parser.stack) >= 1 and parser.list_arcs[parser.stack[-1]] != -1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Gold and Parser Moves\n",
    "1. `parse_moves(parsers: List[ArcEager], moves: Tensor)`: This function takes a list of `ArcEager` parsers and a tensor of moves as input. It iterates over each parser and checks the feasibility of moves based on the provided indices tensor. The function uses the **helper functions** to determine if a move is valid for each parser. The resulting feasible moves are stored in the `list_moves` list, which is then returned.\n",
    "\n",
    "2. `generate_gold(sentence: List[str], gold: List[int])`: This function generates move configurations and heads (arcs) based on the given `sentence` and `gold` list. It creates an `ArcEager` parser and an `Oracle` object using the `sentence` and `gold` lists. The function then performs moves using the parser until the tree is finalized. The resulting moves, configurations, and arcs are returned as lists.\n",
    "\n",
    "Both functions utilize the helper functions and classes provided earlier to handle the parsing process and determine the feasibility of moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_moves(parsers:List[ArcEager], moves:Tensor):\n",
    "    _, indices = tsort(moves, descending=True)\n",
    "    list_moves=[]\n",
    "    for i in range(len(parsers)):\n",
    "        noMove =True \n",
    "        if parsers[i].is_tree_final():\n",
    "           list_moves.append(NOMOVE) \n",
    "           continue\n",
    "        else:\n",
    "            for j in range(4):\n",
    "                if indices[i][j] == LEFT_ARC and is_left_possible(parsers[i]):\n",
    "                    list_moves.append(LEFT_ARC)\n",
    "                    noMove = False;break;\n",
    "                elif indices[i][j] == RIGHT_ARC and is_right_possible(parsers[i]):\n",
    "                    list_moves.append(RIGHT_ARC)\n",
    "                    noMove = False;break;\n",
    "                elif indices[i][j] == REDUCE and is_reduce_possible(parsers[i]):\n",
    "                    list_moves.append(REDUCE)\n",
    "                    noMove = False;break;\n",
    "                elif indices[i][j] == SHIFT and is_shift_possible(parsers[i]) :\n",
    "                    list_moves.append(SHIFT)\n",
    "                    noMove = False;break;\n",
    "        if noMove:\n",
    "            list_moves.append(NOMOVE)\n",
    "    return list_moves\n",
    "\n",
    "def generate_gold(sentence:List[str], gold:List[int]):\n",
    "    '''\n",
    "    Generate moves configurations heads for a given parser and oracle\n",
    "    \n",
    "    input:\n",
    "        parser: ArcEager object\n",
    "        oracle: Oracle object\n",
    "    returns:\n",
    "        moves: list of moves\n",
    "        configurations: list of configurations\n",
    "        arcs: list of heads\n",
    "        \n",
    "    '''\n",
    "    parser:ArcEager=ArcEager(sentence)\n",
    "    oracle:Oracle=Oracle(parser, gold)\n",
    "\n",
    "    while not parser.is_tree_final():\n",
    "        if parser.do_move(oracle.get_next_move()) == NOMOVE:\n",
    "            print(\"ERROR: NOMOVE\")\n",
    "        \n",
    "    return parser.list_moves, parser.list_configurations,  parser.list_arcs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projective check\n",
    "The code `is_projective` checks whether a given dependency parsing tree is projective. It iterates through each word in the tree and examines its head (parent) index. If there are any crossing dependencies within the tree, the function returns False, indicating that the tree is not projective. If no crossing dependencies are found, the function returns True, indicating that the tree is projective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_projective(head):\n",
    "    for i in range(len(head)):\n",
    "        if head[i] == -1:\n",
    "            continue\n",
    "        left = min(i, head[i])\n",
    "        right = max(i, head[i])\n",
    "\n",
    "        for j in range(0, left):\n",
    "            if head[j] > left and head[j] < right:\n",
    "                return False\n",
    "        for j in range(left + 1, right):\n",
    "            if head[j] < left or head[j] > right:\n",
    "                return False\n",
    "        for j in range(right + 1, len(head)):\n",
    "            if head[j] > left and head[j] < right:\n",
    "                return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=256"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Data\n",
    "The code first loads the dataset from the **Universal Dependencies** dataset for English. It splits the dataset into three parts: *training dataset*, *validation dataset*, and *test dataset*.\n",
    "\n",
    "Next, the code filters out the sentences from the datasets that do not have a projective dependency parsing tree. If the resulting tree is projective, the sentence is kept in the dataset; otherwise, it is filtered out.\n",
    "\n",
    "Finally, the code prints the number of sentences remaining in each dataset after the filtering process. It displays the counts for the training dataset, validation dataset, and test dataset separately, indicating the number of sentences that have a projective dependency parsing tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset universal_dependencies (/home/matteo/.cache/huggingface/datasets/universal_dependencies/en_lines/2.7.0/1ac001f0e8a0021f19388e810c94599f3ac13cc45d6b5b8c69f7847b2188bdf7)\n",
      "Found cached dataset universal_dependencies (/home/matteo/.cache/huggingface/datasets/universal_dependencies/en_lines/2.7.0/1ac001f0e8a0021f19388e810c94599f3ac13cc45d6b5b8c69f7847b2188bdf7)\n",
      "Found cached dataset universal_dependencies (/home/matteo/.cache/huggingface/datasets/universal_dependencies/en_lines/2.7.0/1ac001f0e8a0021f19388e810c94599f3ac13cc45d6b5b8c69f7847b2188bdf7)\n",
      "Loading cached processed dataset at /home/matteo/.cache/huggingface/datasets/universal_dependencies/en_lines/2.7.0/1ac001f0e8a0021f19388e810c94599f3ac13cc45d6b5b8c69f7847b2188bdf7/cache-51f72965cf946e1d.arrow\n",
      "Loading cached processed dataset at /home/matteo/.cache/huggingface/datasets/universal_dependencies/en_lines/2.7.0/1ac001f0e8a0021f19388e810c94599f3ac13cc45d6b5b8c69f7847b2188bdf7/cache-f8c488aacd64ac26.arrow\n",
      "Loading cached processed dataset at /home/matteo/.cache/huggingface/datasets/universal_dependencies/en_lines/2.7.0/1ac001f0e8a0021f19388e810c94599f3ac13cc45d6b5b8c69f7847b2188bdf7/cache-bafaf26956452089.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset: 3176, validation_dataset: 1032, test_dataset: 1035\n",
      "PROJECTIVE -> train_dataset: 2922, validation_dataset: 930, test_dataset: 968\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset(\"universal_dependencies\", \"en_lines\", split=\"train\")\n",
    "test_dataset = load_dataset(\"universal_dependencies\", \"en_lines\", split=\"test\")\n",
    "validation_dataset = load_dataset(\"universal_dependencies\", \"en_lines\", split=\"validation\")\n",
    "print(\n",
    "    f\"train_dataset: {len(train_dataset)}, validation_dataset: {len(validation_dataset)}, test_dataset: {len(test_dataset)}\" # type:ignore\n",
    ")  \n",
    "\n",
    "train_dataset = train_dataset.filter(lambda x: is_projective([-1] + list(map(int, x[\"head\"]))))\n",
    "validation_dataset = validation_dataset.filter(lambda x: is_projective([-1] + list(map(int, x[\"head\"]))))\n",
    "test_dataset = test_dataset.filter(lambda x: is_projective([-1] + list(map(int, x[\"head\"]))))\n",
    "print(\n",
    "    f\"PROJECTIVE -> train_dataset: {len(train_dataset)} \\n validation_dataset: {len(validation_dataset)} \\n test_dataset: {len(test_dataset)}\" # type:ignore\n",
    ")  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM Neural Network for Dependency Parsing\n",
    "\n",
    "The code implements a Bidirectional Long Short-Term Memory (BiLSTM) neural network for dependency parsing. The neural network is trained to predict the heads of words in a sentence, which form the dependency tree structure.\n",
    "\n",
    "## Class:\n",
    "1. The `NNParameters` class stores the parameters and hyperparameters for the neural network model. It includes attributes such as batch size, embedding size, LSTM size, number of LSTM layers, MLP output size, number of output classes, dropout rate, learning rate, and number of epochs.\n",
    "\n",
    "2. The `NNData` class represents a data instance used for training and evaluation. It contains attributes for the encoded tokens (`enc_tokens`), configurations (`confs`), moves (`moves`), and heads (`heads`) of a sentence.\n",
    "\n",
    "## Function: \n",
    "1. The `extract_att` function takes a list of `NNData` objects and extracts a specific attribute from each object.\n",
    "2. The `create_dictionary` function creates a word-index mapping based on a dataset. It counts the occurrences of each word in the dataset and includes words that appear at least a specified threshold number of times in the vocabulary. The function returns a dictionary mapping words to their corresponding indices.\n",
    "3. The `process_sample` function processes a sample from the dataset. It takes a sample and an embedding dictionary as input. It adds the `<ROOT>` token to the beginning of the sentence and -1 to the beginning of the head. It encodes the sentence using the provided embedding dictionary. If `get_gold_path` is True, it also generates the gold path and gold moves for the sample.\n",
    "4. The `process_batch` function processes a batch of samples by calling `process_sample` for each sample. It takes a batch of samples, an embedding dictionary, and an optional flag `get_gold_path` as input. The function returns a list of `NNData` objects containing the processed data for each sample in the batch.\n",
    "5. The `train` function performs the training loop for the neural network. It takes the model, dataloader, criterion, and optimizer as input. It iterates over the batches in the dataloader, performs a forward pass through the model, calculates the loss using the specified criterion, performs backpropagation, and updates the model's parameters using the optimizer.\n",
    "6. The `evaluate` function calculates the accuracy of the predicted heads compared to the gold heads. It takes the gold heads and predicted heads as input and computes the accuracy by comparing with **cross entropy**.\n",
    "7. The `test` function evaluates the model's performance on the test dataset. It takes the model and dataloader as input. It iterates over the batches in the dataloader, makes predictions using the model, and collects the gold heads and predicted heads. Finally, it calls the `evaluate` function to calculate the accuracy based on the collected data.\n",
    "\n",
    "The code provides the necessary components to train, evaluate, and test the BiLSTM neural network for dependency parsing. It includes data processing functions, training loop, and evaluation functions to facilitate the training and evaluation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNParameters():\n",
    "  def __init__(self) -> None:\n",
    "      self.BATCH_SIZE = BATCH_SIZE\n",
    "      self.EMBEDDING_SIZE = 200\n",
    "      self.FREEZE = True\n",
    "      self.LSTM_SIZE = 200\n",
    "      self.LSTM_LAYERS = 2\n",
    "      self.MLP_OUT_SIZE = self.LSTM_LAYERS * self.LSTM_SIZE\n",
    "      self.OUT_CLASSES = 4\n",
    "      \n",
    "      self.DROP_OUT = 0.2\n",
    "      self.LR = 0.001\n",
    "      self.EPOCHS = 1\n",
    "\n",
    "nnp = NNParameters()\n",
    "\n",
    "class NNData():\n",
    "  def __init__(self, tokens, confs, moves, heads) -> None:\n",
    "      self.enc_tokens = tokens\n",
    "      self.confs = confs\n",
    "      self.moves = moves\n",
    "      self.heads = heads\n",
    "      #self.dictionary = dictionary\n",
    "\n",
    "def extract_att(data:List[NNData], attribute:str):\n",
    "  return [getattr(d, attribute) for d in data]\n",
    "      \n",
    "\n",
    "def create_dictionary(dataset, threshold: int =3) -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Extract from corpus vocabulary V of unique words that appear at least threshold times.\n",
    "    input:\n",
    "        dataset: list of sentences, each sentence is a list of words\n",
    "        treashold: minimum number of times a word must appear in the corpus to be included in the vocabulary\n",
    "        \n",
    "    output:\n",
    "        map: dictionary of word/index pairs. This is our embedding list\n",
    "    \"\"\"\n",
    "    dic = {}  # dictionary of word counts\n",
    "    for sample in dataset:\n",
    "        for word in sample[\"tokens\"]:\n",
    "            if word in dic:\n",
    "                dic[word] += 1\n",
    "            else:\n",
    "                dic[word] = 1\n",
    "\n",
    "    map = {}  # dictionary of word/index pairs. This is our embedding list\n",
    "    map[\"<pad>\"] = 0\n",
    "    map[\"<ROOT>\"] = 1\n",
    "    map[\"<unk>\"] = 2  # used for words that do not appear in our list\n",
    "\n",
    "    next_indx = 3\n",
    "    for word in dic.keys():\n",
    "        if dic[word] >= threshold:\n",
    "            map[word] = next_indx\n",
    "            next_indx += 1\n",
    "\n",
    "    return map\n",
    "\n",
    "\n",
    "def process_sample(sample, emb_dictionary, get_gold_path=False):\n",
    "    \"\"\"\n",
    "    Process a sample from the dataset\n",
    "    1. Add [\"<ROOT>\"] to the beginning of the sentence and [-1] to the beginning of the head\n",
    "    2. Encode the sentence and the gold path\n",
    "    \n",
    "    \n",
    "    :param         tokens: tokens of a sentence\n",
    "    :param emb_dictionary: dictionary of word/index pairs\n",
    "    :param  get_gold_path: if True, we also return the gold path and gold moves\n",
    "    :return: enc_sentence: encoded tokens of the sentence\n",
    "                gold_path: gold path of the sentence\n",
    "               gold_moves: gold moves of the sentence\n",
    "                     gold: gold heads of the sentence\n",
    "    \"\"\"\n",
    "    sentence = [\"<ROOT>\"] + sample[\"tokens\"]\n",
    "    head = [(-1)] + list(map(int, sample[\"head\"]))  # [int(i) for i in tokens[\"head\"]]\n",
    "\n",
    "    # embedding ids of sentence words\n",
    "    enc_sentence = [\n",
    "        emb_dictionary[word] if word in emb_dictionary else emb_dictionary[\"<unk>\"]\n",
    "        for word in sentence\n",
    "    ]\n",
    "\n",
    "\n",
    "    if get_gold_path:\n",
    "        gold_moves, gold_path, _= generate_gold(sentence, head) # transform matrix from nx3 to 3xn\n",
    "    else:\n",
    "        gold_path, gold_moves = [], []\n",
    "\n",
    "    return enc_sentence, gold_path, gold_moves, head\n",
    "\n",
    "\n",
    "def process_batch(batch:List[List], emb_dictionary:dict[str,int], get_gold_path:bool=False) -> List[NNData]:\n",
    "  pack:List[NNData]=[]\n",
    "\n",
    "  for sample in batch:\n",
    "    s, c, m, h= process_sample(sample, emb_dictionary, get_gold_path=get_gold_path)\n",
    "    pack.append(NNData(s, c, m, h))\n",
    "\n",
    "  return pack \n",
    "\n",
    "def train(model: nn.Module, dataloader, criterion, optimizer):\n",
    "  model.train()\n",
    "  total_loss = 0\n",
    "  count = 1\n",
    "  for batch in dataloader:\n",
    "    print(f\"TRAIN: batch {count}/{len(dataloader):.0f}\")\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    out = model(batch)\n",
    "\n",
    "    moves= extract_att(batch, \"moves\")\n",
    "    labels = torch.tensor(sum(moves, [])).to(\n",
    "        device\n",
    "    )  # sum(moves, []) flatten the array\n",
    "    \n",
    "    loss = criterion(out, labels)\n",
    "    total_loss += loss.item()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    count += 1\n",
    "\n",
    "  return total_loss / count\n",
    "\n",
    "def evaluate(gold:List[List[int]], preds:List[List[int]]):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    for g, p in zip(gold, preds):\n",
    "        for i in range(1, len(g)):\n",
    "            total += 1\n",
    "            if g[i] == p[i]:\n",
    "                correct += 1\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "def test(model, dataloader: torch.utils.data.dataloader):  # type:ignore\n",
    "  model.eval()\n",
    "\n",
    "  gold = []\n",
    "  preds = []\n",
    "  count=0\n",
    "\n",
    "  for batch in dataloader:\n",
    "    print(f\"test: batch {count}/{len(dataloader):.0f}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model.infere(batch)\n",
    "        gold += extract_att(batch, \"heads\")\n",
    "        preds += pred\n",
    "\n",
    "  return evaluate(gold, preds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTMNet Definition\n",
    "\n",
    "The `BiLSTMNet` class is a neural network model for dependency parsing based on Bidirectional Long Short-Term Memory (BiLSTM).\n",
    "\n",
    "## Class: \n",
    "- `get_mlp_input`: Constructs the input for the MLP layers. It takes a list of configurations and hidden states as input and returns the input tensor for the MLP layers.\n",
    "- `mlp_pass`: Performs a forward pass through the MLP layers. It takes the input tensor `x` and returns the output tensor after passing through the MLP layers.\n",
    "- `lstm_pass`: Performs a forward pass through the LSTM layer. It takes the input tensor `x` and returns the output hidden states.\n",
    "- `forward`: Performs the forward pass of the BiLSTMNet model. It takes a batch of `NNData` objects as input and returns the output tensor after passing through the model.\n",
    "- `infer`: Performs inference using the BiLSTMNet model. It takes a batch of data and returns the predicted dependency trees.\n",
    "\n",
    "The code implements the BiLSTMNet model for dependency parsing. It includes the necessary functions to initialize the model, perform forward pass, and conduct inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMNet(nn.Module):\n",
    "  def __init__(self,device, dictionary,  *args, **kwargs) -> None:\n",
    "    super().__init__(*args, **kwargs)\n",
    "    self.device = device\n",
    "    self.embeddings = nn.Embedding(\n",
    "      len(dictionary),\n",
    "      nnp.EMBEDDING_SIZE,\n",
    "      padding_idx=dictionary[\"<pad>\"]\n",
    "    )\n",
    "\n",
    "    self.lstm = nn.LSTM(\n",
    "      nnp.EMBEDDING_SIZE,\n",
    "      nnp.LSTM_SIZE,\n",
    "      num_layers=nnp.LSTM_LAYERS,\n",
    "      bidirectional=True,\n",
    "      dropout=nnp.DROP_OUT,\n",
    "    )\n",
    "    \n",
    "    self.w1 = nn.Linear(2 * nnp.LSTM_LAYERS * nnp.LSTM_SIZE, nnp.MLP_OUT_SIZE, bias=True)\n",
    "    self.activation = nn.Tanh()\n",
    "    self.w2 = nn.Linear(nnp.MLP_OUT_SIZE, nnp.OUT_CLASSES, bias=True)\n",
    "    self.softmax = nn.Softmax(dim=-1)\n",
    "    self.dropout = nn.Dropout(nnp.DROP_OUT)\n",
    "  \n",
    "  def get_mlp_input(self, configurations, h):\n",
    "      mlp_input = []\n",
    "      zero_tensor = torch.zeros(\n",
    "          2 * nnp.LSTM_SIZE, requires_grad=False, device=self.device\n",
    "      )\n",
    "      for i in range(len(configurations)):\n",
    "          for j in configurations[i]:  # for each configuration of a sentence\n",
    "              mlp_input.append(\n",
    "                  torch.cat(\n",
    "                      [\n",
    "                          zero_tensor if j[0] == -1 else h[j[0]][i],\n",
    "                          zero_tensor if j[1] == -1 else h[j[1]][i],\n",
    "                      ]\n",
    "                  )\n",
    "              )\n",
    "      mlp_input = torch.stack(mlp_input).to(self.device)\n",
    "      return mlp_input\n",
    "\n",
    "  def mlp_pass(self, x):\n",
    "      return self.softmax(\n",
    "          self.w2(self.dropout(self.activation(self.w1(self.dropout(x)))))\n",
    "      )\n",
    "\n",
    "  def lstm_pass(self, x):\n",
    "    x = torch.nn.utils.rnn.pack_sequence(x, enforce_sorted=False)\n",
    "    h, _ = self.lstm(x)\n",
    "    h, _ = torch.nn.utils.rnn.pad_packed_sequence(h)\n",
    "    return h\n",
    "  \n",
    "  def forward(self, batch:List[NNData]):\n",
    "    tokens = extract_att(batch, \"enc_tokens\")\n",
    "    x = [self.dropout(self.embeddings(torch.tensor(t).to(self.device))) for t in tokens]\n",
    "\n",
    "    h = self.lstm_pass(x)\n",
    "    \n",
    "    configurations:List[List[Tuple[int,int]]] = extract_att(batch, \"confs\")\n",
    "    mlp_input = self.get_mlp_input(configurations, h)\n",
    "    out = self.mlp_pass(mlp_input)\n",
    "    return out\n",
    "\n",
    "  def infere(self, batch):\n",
    "    start_time=time.time()\n",
    "    tokens=extract_att(batch, \"enc_tokens\")\n",
    "    parsers: List[ArcEager] = [ArcEager(t) for t in tokens]\n",
    "\n",
    "    x = [self.embeddings(torch.tensor(t).to(self.device)) for t in tokens]\n",
    "    h = self.lstm_pass(x)\n",
    "    print(\"time lstm\", time.time()-start_time)\n",
    "\n",
    "    start_time=time.time()\n",
    "    is_final = [False] \n",
    "    while not all(is_final):\n",
    "      # get the current configuration and score next moves\n",
    "      configurations = [[p.get_configuration_now()] for p in parsers]\n",
    "      mlp_input = self.get_mlp_input(configurations, h)\n",
    "      mlp_out = self.mlp_pass(mlp_input)\n",
    "      # take the next parsing step\n",
    "      list_moves= parse_moves(parsers, mlp_out)\n",
    "      for i,m in enumerate(list_moves):\n",
    "          parsers[i].do_move(m)\n",
    "      is_final=[t.is_tree_final() for t in parsers]\n",
    "          \n",
    "    print(\"time parse\", time.time()-start_time)\n",
    "\n",
    "    # return the predicted dependency tree\n",
    "    return [parser.list_arcs for parser in parsers]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader\n",
    "The provided code sets up data loaders for training, validation, and testing datasets using the `torch.utils.data.DataLoader` class.\n",
    "\n",
    "1. `create_dictionary`: Creates a word-index mapping dictionary based on the training dataset.\n",
    "2. `torch.utils.data.DataLoader`: Creates a data loader for the training, test, validation dataset with batch size, shuffling, and data processing using `process_batch` function with gold path and moves.\n",
    "\n",
    "These data loaders facilitate the training, validation, and testing processes by providing batches of data, shuffling the data, and applying the necessary data processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary= create_dictionary(train_dataset)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "  train_dataset,\n",
    "  batch_size=nnp.BATCH_SIZE, \n",
    "  shuffle=True,\n",
    "  collate_fn=lambda x: process_batch(x, dictionary, get_gold_path=True)\n",
    ")\n",
    "\n",
    "validation_dataloader= torch.utils.data.DataLoader(\n",
    "  validation_dataset,\n",
    "  batch_size=nnp.BATCH_SIZE,\n",
    "  shuffle=True,\n",
    "  collate_fn=lambda x: process_batch(x, dictionary, get_gold_path=True)\n",
    ")\n",
    "\n",
    "test_dataloader= torch.utils.data.DataLoader(\n",
    "  test_dataset,\n",
    "  batch_size=nnp.BATCH_SIZE,\n",
    "  shuffle=True,\n",
    "  collate_fn=lambda x: process_batch(x, dictionary, get_gold_path=False)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traing and evaluation\n",
    "The code trains the BiLSTMNet model using the training dataset and evaluates the model's performance on the validation dataset. It uses the `train` and `evaluate` functions to perform the training and evaluation processes.\n",
    "`criterion` used to calculate the loss is **Cross Entropy**.\n",
    "`optimizer` used to update the model's parameters is **Adam**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 0\n",
      "TRAIN: batch 1/12\n",
      "TRAIN: batch 2/12\n"
     ]
    }
   ],
   "source": [
    "model = BiLSTMNet(device, dictionary).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=nnp.LR)\n",
    "\n",
    "for epoch in range(nnp.EPOCHS):\n",
    "  print(\"Starting Epoch\", epoch)\n",
    "  # torch.load(f\"bilstm_e{epoch+1}.pt\")\n",
    "  avg_train_loss = train(model, train_dataloader, criterion, optimizer)\n",
    "  val_uas = test(model, validation_dataloader)\n",
    "\n",
    "  log = f\"Epoch: {epoch:3d} | avg_train_loss: {avg_train_loss:5.3f} | dev_uas: {val_uas:5.3f} |\"\n",
    "  print(log)\n",
    "\n",
    "  # save the model on pytorch format\n",
    "  torch.save(model.state_dict(), f\"bilstm_e{epoch+1}.pt\")\n",
    "\n",
    "test_uas = test(model, test_dataloader)\n",
    "log = \"test_uas: {:5.3f}\".format(test_uas)\n",
    "print(log)\n",
    "train(model, train_dataloader, criterion, optimizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNParameters():\n",
    "  def __init__(self) -> None:\n",
    "      self.BATCH_SIZE = BATCH_SIZE \n",
    "      self.BERT_SIZE = 768\n",
    "      self.EMBEDDING_SIZE = self.BERT_SIZE\n",
    "      self.DIM_CONFIG = 2\n",
    "      self.MLP1_IN_SIZE = self.DIM_CONFIG * self.EMBEDDING_SIZE\n",
    "      self.MLP2_IN_SIZE = 300\n",
    "      self.OUT_CLASSES = 4\n",
    "      self.FREEZE = True\n",
    "      self.DROP_OUT = 0.2\n",
    "      self.LR = 0.01\n",
    "      self.EPOCHS = 1\n",
    "\n",
    "nnp = NNParameters()\n",
    "\n",
    "class NNData():\n",
    "  def __init__(self,sentence, confs, moves, heads, subw2word_idx) -> None:\n",
    "      self.sentence = sentence\n",
    "      self.confs = confs\n",
    "      self.moves = moves\n",
    "      self.heads = heads\n",
    "      self.subw2word_idx = subw2word_idx\n",
    "\n",
    "def extract_att(data:List[NNData], attribute:str):\n",
    "  return [getattr(d, attribute) for d in data]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer.add_tokens([\"<ROOT>\", \"<EMPTY>\"], special_tokens=True)\n",
    "\n",
    "def calcualate_subw2word_idx(subw_idx):\n",
    "  tokens=tokenizer.convert_ids_to_tokens(subw_idx)\n",
    "  i=0\n",
    "  o=[]\n",
    "  while i<len(tokens):\n",
    "    t=[]\n",
    "    t.append(i)\n",
    "    i+=1\n",
    "    while i<len(tokens) and tokens[i].startswith(\"##\"):\n",
    "      t.append(i)\n",
    "      i+=1\n",
    "    o.append(t)\n",
    "  return o\n",
    "\n",
    "def process_sample(sample, sentence, iid, get_gold_path=False):\n",
    "    \"\"\"\n",
    "    Process a sample from the dataset\n",
    "    1. Add [\"<ROOT>\"] to the beginning of the sentence and [-1] to the beginning of the head\n",
    "    2. Encode the sentence and the gold path\n",
    "    \n",
    "    \n",
    "    :param         tokens: tokens of a sentence\n",
    "    :param emb_dictionary: dictionary of word/index pairs\n",
    "    :param  get_gold_path: if True, we also return the gold path and gold moves\n",
    "    :return: enc_sentence: encoded tokens of the sentence\n",
    "                gold_path: gold path of the sentence\n",
    "               gold_moves: gold moves of the sentence\n",
    "                     gold: gold heads of the sentence\n",
    "    \"\"\"\n",
    "\n",
    "    head = [(-1)] + list(map(int, sample[\"head\"]))  # [int(i) for i in tokens[\"head\"]]\n",
    "\n",
    "    # embedding ids of sentence words\n",
    "    subw2word_idx=calcualate_subw2word_idx(iid)\n",
    "\n",
    "    if get_gold_path:\n",
    "        gold_moves, gold_path, _= generate_gold(sentence, head) # transform matrix from nx3 to 3xn\n",
    "    else:\n",
    "        gold_path, gold_moves = [], []\n",
    "\n",
    "    return head, gold_path, gold_moves, subw2word_idx\n",
    "\n",
    "\n",
    "def process_batch(batch:List[List], tokenizer, get_gold_path:bool=False) :\n",
    "  pack:List[NNData]=[]\n",
    "  sentences=[[\"<ROOT>\"]+ bd[\"tokens\"] for bd in batch]\n",
    "\n",
    "  ## Tokenizer -> get token_ids, attention_mask and token_type_ids\n",
    "  output_tokenizer= tokenizer(\n",
    "      [\"<ROOT> \"+ bd[\"text\"] for bd in batch],\n",
    "      padding=True, \n",
    "      return_tensors=\"pt\",\n",
    "      add_special_tokens=False \n",
    "  )\n",
    "\n",
    "  token_ids:List[List[int]]=output_tokenizer[\"input_ids\"]\n",
    "  attention_mask=output_tokenizer[\"attention_mask\"]\n",
    "  token_types_ids=output_tokenizer[\"token_type_ids\"] \n",
    "  ###########\n",
    "  ## What is left? heads, gold_path, gold_moves, subw2word_idx\n",
    "  ## What i need? sample -> heads, original sentence -> golds, input_ids -> subw2word_idx\n",
    "  for sample, sentence, iid in zip(batch,sentences,token_ids):\n",
    "    head, configuration, moves, s2w= process_sample(sample, sentence,iid, get_gold_path=get_gold_path)\n",
    "    pack.append(NNData(sentence, configuration, moves, head, s2w))\n",
    "\n",
    "  return output_tokenizer, pack"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Network definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTNet(nn.Module):\n",
    "  def __init__(self,device, tokenizer,  *args, **kwargs) -> None:\n",
    "    super().__init__(*args, **kwargs)\n",
    "    self.device = device\n",
    "    self.embeddings = nn.Embedding(\n",
    "      len(tokenizer),\n",
    "      nnp.EMBEDDING_SIZE,\n",
    "      padding_idx=0\n",
    "    )\n",
    "\n",
    "    \n",
    "    self.bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "    self.bert.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # Freeze bert layers\n",
    "    if nnp.FREEZE:\n",
    "      for param in self.bert.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    self.w1 = nn.Linear(nnp.MLP1_IN_SIZE, nnp.MLP2_IN_SIZE, bias=True)\n",
    "    self.activation = nn.Tanh()\n",
    "    self.w2 = nn.Linear(nnp.MLP2_IN_SIZE, nnp.OUT_CLASSES, bias=True)\n",
    "    self.softmax = nn.Softmax(dim=-1)\n",
    "    self.dropout = nn.Dropout(nnp.DROP_OUT)\n",
    "  \n",
    "  def get_embedding(self, h, idx):\n",
    "      return torch.mean(h[idx], dim=0)\n",
    "\n",
    "  def get_mlp_input(self, configurations, subw2idx, h):\n",
    "      mlp_input = []\n",
    "      zero_tensor = torch.zeros(\n",
    "          nnp.BERT_SIZE, requires_grad=False, device=self.device\n",
    "      )\n",
    "      for i in range(len(configurations)):\n",
    "          for j in configurations[i]:  # for each configuration of a sentence\n",
    "              mlp_input.append(\n",
    "                  torch.cat(\n",
    "                      [\n",
    "                          zero_tensor if j[0] == -1 else self.get_embedding(h[i], subw2idx[i][j[0]]),\n",
    "                          zero_tensor if j[1] == -1 else self.get_embedding(h[i], subw2idx[i][j[1]]),\n",
    "                      ]\n",
    "                  )\n",
    "              )\n",
    "      mlp_input = torch.stack(mlp_input).to(self.device)\n",
    "      return mlp_input\n",
    "\n",
    "  def mlp_pass(self, x):\n",
    "      return self.softmax(\n",
    "          self.w2(self.dropout(self.activation(self.w1(self.dropout(x)))))\n",
    "      )\n",
    "\n",
    "  \n",
    "  def forward(self, batch:Tuple[BatchEncoding,List[NNData]]):\n",
    "    output_tokenizer = batch[0].to(self.device)\n",
    "    input_ids= output_tokenizer[\"input_ids\"]\n",
    "    attention_mask= output_tokenizer[\"attention_mask\"] \n",
    "    input_ids=input_ids.to(self.device)\n",
    "    attention_mask=attention_mask.to(self.device)\n",
    "\n",
    "    h = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state.to(self.device)\n",
    "    \n",
    "    nndata=batch[1]\n",
    "    configurations = extract_att(nndata, \"confs\")\n",
    "    subw2idx = extract_att(nndata, \"subw2word_idx\")\n",
    "    mlp_input = self.get_mlp_input(configurations,subw2idx, h)\n",
    "\n",
    "    out = self.mlp_pass(mlp_input)\n",
    "    return out\n",
    "\n",
    "  def infere(self, batch):\n",
    "    output_tokenizer, nndata= batch\n",
    "    output_tokenizer=output_tokenizer.to(self.device)\n",
    "    input_ids= output_tokenizer[\"input_ids\"]\n",
    "    attention_mask= output_tokenizer[\"attention_mask\"] \n",
    "    input_ids=input_ids.to(self.device)\n",
    "    attention_mask=attention_mask.to(self.device)\n",
    "\n",
    "    h = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state.to(self.device)\n",
    "\n",
    "    tokens = extract_att(nndata, \"sentence\")\n",
    "    parsers: List[ArcEager] = [ArcEager(t) for t in tokens]\n",
    "\n",
    "\n",
    "    subw2idx = extract_att(nndata, \"subw2word_idx\")\n",
    "    is_final = [False] \n",
    "    while not all(is_final):\n",
    "      # get the current configuration and score next moves\n",
    "      configurations = [[p.get_configuration_now()] for p in parsers]\n",
    "      mlp_input = self.get_mlp_input(configurations, subw2idx, h)\n",
    "      mlp_out = self.mlp_pass(mlp_input)\n",
    "      # take the next parsing step\n",
    "      list_moves= parse_moves(parsers, mlp_out)\n",
    "      for i,m in enumerate(list_moves):\n",
    "          parsers[i].do_move(m)\n",
    "      is_final=[t.is_tree_final() for t in parsers]\n",
    "          \n",
    "\n",
    "    # return the predicted dependency tree\n",
    "    return [parser.list_arcs for parser in parsers]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "  train_dataset,\n",
    "  batch_size=nnp.BATCH_SIZE, \n",
    "  shuffle=True,\n",
    "  collate_fn=lambda x: process_batch(x, tokenizer, get_gold_path=True)\n",
    ")\n",
    "validation_dataloader= torch.utils.data.DataLoader(\n",
    "  validation_dataset,\n",
    "  batch_size=nnp.BATCH_SIZE,\n",
    "  shuffle=True,\n",
    "  collate_fn=lambda x: process_batch(x,tokenizer, get_gold_path=True)\n",
    ")\n",
    "test_dataloader= torch.utils.data.DataLoader(\n",
    "  test_dataset,\n",
    "  batch_size=nnp.BATCH_SIZE,\n",
    "  shuffle=True,\n",
    "  collate_fn=lambda x: process_batch(x, tokenizer, get_gold_path=False)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " DO SOMETHING !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTNet(device, tokenizer).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=nnp.LR)\n",
    "\n",
    "for epoch in range(nnp.EPOCHS):\n",
    "  print(\"Starting Epoch\", epoch)\n",
    "  # torch.load(f\"bilstm_e{epoch+1}.pt\")\n",
    "  avg_train_loss = train(model, train_dataloader, criterion, optimizer)\n",
    "  if epoch % 5 == 0:\n",
    "    val_uas = test(model, validation_dataloader)\n",
    "  else:\n",
    "    val_uas = -1\n",
    "    \n",
    "\n",
    "  log = f\"Epoch: {epoch:3d} | avg_train_loss: {avg_train_loss:5.3f} | dev_uas: {val_uas:5.3f} |\"\n",
    "  print(log)\n",
    "\n",
    "  # save the model on pytorch format\n",
    "  torch.save(model.state_dict(), f\"bilstm_e{epoch+1}.pt\")\n",
    "\n",
    "test_uas = test(model, test_dataloader)\n",
    "log = \"test_uas: {:5.3f}\".format(test_uas)\n",
    "print(log)\n",
    "train(model, train_dataloader, criterion, optimizer)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
